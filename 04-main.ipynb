{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "from ast import literal_eval\n",
    "\n",
    "# viz\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# parallel\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "# custom\n",
    "import importlib\n",
    "import proj_funs\n",
    "importlib.reload(proj_funs)\n",
    "from proj_funs import extract_subjects, read_saved, save_covers\n",
    "\n",
    "# ML\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torch import nn\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torchvision.models.resnet import ResNet, BasicBlock\n",
    "\n",
    "# count cpus\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import subject data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = proj_funs.read_saved(\"df_clean_uniqkey.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lables = list(set(np.concatenate(df.subjects.values).flat))\n",
    "lables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10**4+2000\n",
    "df_sample = df.sample(n=N, random_state=1).reset_index(drop=True)\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Img processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = np.ravel(df_sample.cover.values)\n",
    "image_size = \"M\"\n",
    "#save_covers(image_ids, image_size)\n",
    "Parallel(n_jobs=100)(delayed(save_covers)([i], image_size) for i in image_ids)\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_folder = 'covers'\n",
    "size = \"M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OLDataset(Dataset):\n",
    "    \"\"\"subjects+cover dataset object definition\n",
    "    methods: indexing, len\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path, samples, transforms):\n",
    "        self.transforms = transforms\n",
    "        self.classes = lables\n",
    "\n",
    "        self.imgs = []\n",
    "        self.annos = []\n",
    "        self.data_path = data_path\n",
    "        for k, sample in samples.iterrows():\n",
    "            self.imgs.append(str(sample['cover'])+\"-\"+size+\".jpg\")\n",
    "            self.annos.append(sample['subjects'])\n",
    "        for item_id in range(len(self.annos)):\n",
    "            item = self.annos[item_id]\n",
    "            vector = [cls in item for cls in self.classes]\n",
    "            self.annos[item_id] = np.array(vector, dtype=float)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        anno = self.annos[item]\n",
    "        img_path = os.path.join(self.data_path, self.imgs[item])\n",
    "        img = Image.open(img_path)\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        return img, anno\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pct = 0.8334\n",
    "test_pct = 0.16667/2\n",
    "val_pct = 1-train_pct-test_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = OLDataset(\"covers\", df_sample[:int(len(df_sample)*train_pct)+1], None)\n",
    "dataset_val = OLDataset(\"covers\", df_sample[int(len(df_sample)*train_pct)+1:int(len(df_sample)*train_pct)+1+int(len(df_sample)*val_pct)+1], None)\n",
    "dataset_test = OLDataset(\"covers\", df_sample[int(len(df_sample)*train_pct)+1+int(len(df_sample)*val_pct)+1:], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_val[0])\n",
    "\n",
    "# A simple function for visualization.\n",
    "def show_sample(img, binary_img_labels):\n",
    "    # Convert the binary labels back to the text representation.    \n",
    "    img_labels = np.array(dataset_val.classes)[np.argwhere(binary_img_labels > 0)[:, 0]]\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"{}\".format(', '.join(img_labels)))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "for sample_id in range(50,70):\n",
    "    show_sample(*dataset_val[sample_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate label distribution for the entire dataset (train + test)\n",
    "samples = dataset_val.annos + dataset_train.annos\n",
    "samples = np.array(samples)\n",
    "with np.printoptions(precision=3, suppress=True):\n",
    "    class_counts = np.sum(samples, axis=0)\n",
    "    # Sort labels according to their frequency in the dataset.\n",
    "    sorted_ids = np.array([i[0] for i in sorted(enumerate(class_counts), key=lambda x: x[1])], dtype=int)\n",
    "    print('Label distribution (count, class name):', list(zip(class_counts[sorted_ids].astype(int), np.array(dataset_val.classes)[sorted_ids])))\n",
    "    plt.barh(range(len(dataset_val.classes)), width=class_counts[sorted_ids])\n",
    "    plt.yticks(range(len(dataset_val.classes)), np.array(dataset_val.classes)[sorted_ids])\n",
    "    plt.gca().margins(y=0)\n",
    "    plt.grid()\n",
    "    plt.title('Label distribution')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test preprocessing\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "print(tuple(np.array(np.array(mean)*255).tolist()))\n",
    "\n",
    "# Train preprocessing\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ColorJitter(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "https://github.com/spmallick/learnopencv/tree/master/PyTorch-Multi-Label-Image-Classification-Image-Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the dataloaders for training\n",
    "batch_size = 1\n",
    "\n",
    "test_dataset = OLDataset(\"covers\", df_sample[:int(len(df_sample)*train_pct)+1], val_transform)\n",
    "train_dataset = OLDataset(\"covers\", df_sample[int(len(df_sample)*train_pct)+1+int(len(df_sample)*val_pct)+1:], train_transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "num_train_batches = int(np.ceil(len(train_dataset) / batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/vision/0.8/_modules/torchvision/models/resnet.html\n",
    "# based on pytorch resnet18 implementation\n",
    "# rmv linear layers\n",
    "# add final convolutional layer\n",
    "# and a Sigmoid instead of a default Softmax.\n",
    "class FCResNet18(ResNet):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__(BasicBlock, [2, 2, 2, 2])\n",
    "        self.sigm = nn.Sigmoid()\n",
    "        self.final_conv = nn.Conv2d(in_channels=512, out_channels=22, kernel_size=1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=512, out_features=n_classes)\n",
    "        )\n",
    "\n",
    "    def _forward_impl(self, x):\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = self.final_conv(x)\n",
    "        x = torch.max(torch.max(x,0).values,1).values\n",
    "        x = torch.transpose(x,1,0)\n",
    "        # x = torch.flatten(x, 0)\n",
    "        # x = self.fc(x)  # (1x1000 x 1000x22) add linear layers?\n",
    "        x = self.sigm(x)\n",
    "\n",
    "        return x\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sigmoid()\n",
    "input = torch.randn(2)\n",
    "m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 22, 1, 1)\n",
    "# print(x)\n",
    "x = torch.max(torch.max(x,0).values,1).values\n",
    "x = torch.transpose(x,1,0)\n",
    "# torch.max(x,0).values.shape\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training parameters & metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the training parameters.\n",
    "lr = 1e-4 # Learning rate\n",
    "test_freq = 200 # Test model frequency (iterations)\n",
    "max_epoch_number = 35 # Max num of training epochs \n",
    "\n",
    "# Initialize the model\n",
    "model = FCResNet18(len(lables))\n",
    "# model.load_state_dict(models.resnet18(pretrained=True).state_dict())\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use threshold to define predicted labels and invoke sklearn's metrics with different averaging strategies.\n",
    "def calculate_metrics(pred, target, threshold=0.5):\n",
    "    pred = np.array(pred > threshold, dtype=float)\n",
    "    return {'micro/precision': precision_score(y_true=target, y_pred=pred, average='micro'),\n",
    "            'micro/recall': recall_score(y_true=target, y_pred=pred, average='micro'),\n",
    "            'micro/f1': f1_score(y_true=target, y_pred=pred, average='micro'),\n",
    "            'macro/precision': precision_score(y_true=target, y_pred=pred, average='macro'),\n",
    "            'macro/recall': recall_score(y_true=target, y_pred=pred, average='macro'),\n",
    "            'macro/f1': f1_score(y_true=target, y_pred=pred, average='macro'),\n",
    "            'samples/precision': precision_score(y_true=target, y_pred=pred, average='samples'),\n",
    "            'samples/recall': recall_score(y_true=target, y_pred=pred, average='samples'),\n",
    "            'samples/f1': f1_score(y_true=target, y_pred=pred, average='samples'),\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "model.train()\n",
    "epoch = 0\n",
    "iteration = 0\n",
    "while True:\n",
    "    batch_losses = []\n",
    "    for imgs, targets in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        model_result = model(imgs)\n",
    "        loss = criterion(model_result, targets.type(torch.float))\n",
    "\n",
    "        batch_loss_value = loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(batch_loss_value)\n",
    "        with torch.no_grad():\n",
    "            result = calculate_metrics(model_result, targets)\n",
    "\n",
    "        if iteration % test_freq == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                model_result = []\n",
    "                targets = []\n",
    "                for imgs, batch_targets in test_dataloader:\n",
    "                    imgs = imgs\n",
    "                    model_batch_result = model(imgs)\n",
    "                    model_result.extend(model_batch_result)\n",
    "                    targets.extend(batch_targets)\n",
    "\n",
    "            result = calculate_metrics(np.array(model_result), np.array(targets))\n",
    "            print(\"epoch:{:2d} iter:{:3d} test: \"\n",
    "                  \"micro f1: {:.3f} \"\n",
    "                  \"macro f1: {:.3f} \"\n",
    "                  \"samples f1: {:.3f}\".format(epoch, iteration,\n",
    "                                              result['micro/f1'],\n",
    "                                              result['macro/f1'],\n",
    "                                              result['samples/f1']))\n",
    "\n",
    "            model.train()\n",
    "        iteration += 1\n",
    "\n",
    "    loss_value = np.mean(batch_losses)\n",
    "    print(\"epoch:{:2d} iter:{:3d} train: loss:{:.3f}\".format(epoch, iteration, loss_value))\n",
    "    epoch += 1\n",
    "    if max_epoch_number < epoch:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on the test data\n",
    "model.eval()\n",
    "for sample_id in [1,2,3,4,6]:\n",
    "    test_img, test_labels = test_dataset[sample_id]\n",
    "    test_img_path = os.path.join(img_folder, test_dataset.imgs[sample_id])\n",
    "    with torch.no_grad():\n",
    "        raw_pred = model(test_img.unsqueeze(0))[0]\n",
    "        raw_pred = np.array(raw_pred > 0.5, dtype=float)\n",
    "\n",
    "    predicted_labels = np.array(dataset_val.classes)[np.argwhere(raw_pred > 0)[:, 0]]\n",
    "    if not len(predicted_labels):\n",
    "        predicted_labels = ['no predictions']\n",
    "    img_labels = np.array(dataset_val.classes)[np.argwhere(test_labels > 0)[:, 0]]\n",
    "    plt.imshow(Image.open(test_img_path))\n",
    "    plt.title(\"Predicted labels: {} \\nGT labels: {}\".format(', '.join(predicted_labels), ', '.join(img_labels)))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "573805b6bf94dfdb6e23a5ddf9205e43e813018f3ce9b293891b8d65ab8de9da"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
